import time
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options

from config import BASE_URL, PAGES_TO_SCRAPE
from utils import clean_text


def setup_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    return webdriver.Chrome(options=options)


def scrape_page(driver, page_number):
    url = f"{BASE_URL}?page={page_number}"
    driver.get(url)
    time.sleep(3)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    items = soup.select('.product-item')

    records = []
    for item in items:
        title = clean_text(item.select_one('.product-title').text)
        price = clean_text(item.select_one('.product-price').text)
        rating = clean_text(item.select_one('.product-rating').text) \
                 if item.select_one('.product-rating') else "N/A"
        link = item.select_one('a')['href']

        records.append({
            "title": title,
            "price": price,
            "rating": rating,
            "url": link
        })

    return records


def main():
    driver = setup_driver()
    all_records = []

    for page in range(1, PAGES_TO_SCRAPE + 1):
        print(f"Scraping page {page}...")
        page_records = scrape_page(driver, page)
        all_records.extend(page_records)

    df = pd.DataFrame(all_records)
    df.to_csv("output/scraped_data.csv", index=False)

    print("Scraping completed. Data saved to output/scraped_data.csv")
    driver.quit()


if __name__ == "__main__":
    main()
